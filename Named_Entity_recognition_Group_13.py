# -*- coding: utf-8 -*-
"""Final_Named_Entity_Recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J8WVmUNkyaPQB0eVUk-ti-V_jNIkAeDv
"""

from google.colab import drive

drive.mount('/content/drive')

"""# Data Preprocessing

## Load Dataset
"""

!pip install evaluate
!pip install gradio
import pandas as pd
from datasets import Dataset
from ast import literal_eval
from transformers import AutoTokenizer
import numpy as np
from evaluate import load
import gradio as gr

import pandas as pd

# Use the raw file link
url = 'https://raw.githubusercontent.com/JordenBong/NLP-Assignment/main/dataset/ner.csv'

# Load your CSV file
df = pd.read_csv(url)

# Inspect the dataframe to understand its structure
df.head()

"""## Change the Representation in Columns"""

# Try to convert POS and Tag to list representation instead of string
print("POS Type (Before): ", type(df["POS"][0]))
print("Tag Type (Before): ", type(df["Tag"][0]))

df['POS'] = df['POS'].apply(literal_eval)
df['Tag'] = df['Tag'].apply(literal_eval)

print("POS Type (After): ", type(df["POS"][0]))
print("Tag Type (After): ", type(df["Tag"][0]))

"""## Tokenize the Sentence"""

# Preprocess 1: Tokenize the sentence and store in another column called Token
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt_tab')

df['Token'] = df['Sentence'].map(word_tokenize)

df.head()

"""## Map the BIO Tags to dslim's Tags (Pre-trained model)"""

# Preprocess 2: Map the BIO entity type in the NER Tag - Tag mapping from current tags to dslim's tags
tag_mapping = {
    'B-art': 'B-MISC', 'I-art': 'I-MISC',
    'B-eve': 'B-MISC', 'I-eve': 'I-MISC',
    'B-geo': 'B-LOC', 'I-geo': 'I-LOC',
    'B-gpe': 'B-LOC', 'I-gpe': 'I-LOC',
    'B-nat': 'B-MISC', 'I-nat': 'I-MISC',
    'B-org': 'B-ORG', 'I-org': 'I-ORG',
    'B-per': 'B-PER', 'I-per': 'I-PER',
    'B-tim': 'B-MISC', 'I-tim': 'I-MISC',
    'O': 'O'
}

def map_tags(ner_tags):
    new_tags = [tag_mapping[tag] for tag in ner_tags]
    return new_tags

df['New_Tag'] = df['Tag'].apply(map_tags)

df.head()

"""## Check any Mismatch Length between Token and POS or Ner Tags"""

# Checking Function to identify problematic rows
def is_valid_row(row):
    token = row['Token']
    pos = row['POS']
    tag = row['New_Tag']

    return len(token) == len(pos) and len(token) == len(tag)

# Create mask for valid rows
valid_mask = df.apply(is_valid_row, axis=1)
problematic_df = df[~valid_mask]
clean_df = df[valid_mask]

print(f"Total rows: {len(df)}")
print(f"Problematic rows: {len(problematic_df)}")
print(f"Clean rows: {len(clean_df)}")

"""## Save the Separate Dataset
- Direct use clean_df for training
- problematic_df is only 0.0046 % out of total instances
- manual correction for problematic_df if required
"""

# Save problematic instances for potential manual correction
problematic_df.to_csv('problematic_instances.csv', index=False)
clean_df.to_csv('clean_instances.csv', index=False)

"""## Convert to HuggingFace Dataset"""

# Convert to HuggingFace Dataset
dataset = Dataset.from_pandas(clean_df)

"""## Final Verification"""

# Final verification
def verify_dataset(dataset):
    errors = 0
    for example in dataset:
        if len(example['Token']) != len(example['POS']) or len(example['Token']) != len(example['New_Tag']):
            errors += 1
    print(f"Clean dataset contains {errors} length mismatch errors out of {len(dataset)} examples")

verify_dataset(dataset)

"""# Start Training

# Load Tokenizer
"""

model_name = "dslim/bert-base-NER"
tokenizer = AutoTokenizer.from_pretrained(model_name)

"""# Define Label List"""

label_list = sorted(list(set(tag for row in dataset["New_Tag"] for tag in row)))
label_to_id = {label: i for i, label in enumerate(label_list)}
id_to_label = {i: label for label, i in label_to_id.items()}

def tokenize_and_align_labels(example):
    tokenized_inputs = tokenizer(
        example["Token"],
        is_split_into_words=True,
        truncation=True,
        padding='max_length',
        max_length=128,
        return_offsets_mapping=True  # Useful for debugging
    )

    labels = []
    word_ids = tokenized_inputs.word_ids()  # Maps subwords to original word indices

    previous_word_idx = None
    for word_idx in word_ids:
        if word_idx is None:
            labels.append(-100)  # Ignore tokens like [CLS], [SEP], padding
        elif word_idx != previous_word_idx:
            labels.append(label_to_id[example["New_Tag"][word_idx]])
        else:
            labels.append(-100)  # Only label first subword token
        previous_word_idx = word_idx

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

"""# Apply to Dataset"""

# Apply to dataset
tokenized_dataset = dataset.map(tokenize_and_align_labels)

"""# Prepare Data for Training

# Split Data (~81% train, ~9% validation, 10% test)
"""

tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)
train_val = tokenized_dataset["train"]
test_dataset = tokenized_dataset["test"]

# Then split train_val into train and validation
split_train_val = train_val.train_test_split(test_size=0.1)
train_dataset = split_train_val["train"]
val_dataset = split_train_val["test"]

"""# Define model & print summary"""

!pip install torchinfo
from transformers import BertForTokenClassification
from torchinfo import summary
import torch

model = BertForTokenClassification.from_pretrained(
    model_name,
    num_labels=len(label_list),
    id2label = id_to_label,
    label2id = label_to_id
)



example = tokenized_dataset['train'][0]
input_ids = torch.tensor([example['input_ids']])
attention_mask = torch.tensor([example['attention_mask']])

inputs = {
    "input_ids": input_ids,
    "attention_mask": attention_mask
}

summary(model, input_data=inputs)

"""# Setup Trainer"""

#!pip install --upgrade transformers
from transformers import TrainingArguments, Trainer
import torch

print("CUDA available:", torch.cuda.is_available())
print("Device name:", torch.cuda.get_device_name() if torch.cuda.is_available() else "No GPU")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    learning_rate=5e-5,
    warmup_steps=500,
    weight_decay=0.01,
    fp16=True,
    report_to=[],
)

from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)

# 4. Trainer setup
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator = data_collator
)

"""# Train the model"""

!export WANDB_DISABLED=true
trainer.train()

"""# Save Model"""

trainer.save_model("/content/drive/MyDrive/NLP/my-ner-model")

"""# To Evaluate using seqeval"""

from sklearn.metrics import classification_report
from datasets import load_metric
import numpy as np

!pip install seqeval
from seqeval.metrics import classification_report as seqeval_report
from seqeval.metrics import precision_score, recall_score, f1_score

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    true_labels = [
        [id_to_label[label] for label in sent if label != -100]
        for sent in labels
    ]
    true_predictions = [
        [id_to_label[pred] for pred, label in zip(sent_pred, sent_label) if label != -100]
        for sent_pred, sent_label in zip(predictions, labels)
    ]

    return {
        "precision": precision_score(true_labels, true_predictions),
        "recall": recall_score(true_labels, true_predictions),
        "f1": f1_score(true_labels, true_predictions),
    }

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# Evaluate on test dataset (final evaluation)
test_results = trainer.evaluate(eval_dataset=test_dataset)
print("\nFinal Test Set Metrics:")
print(f"Precision: {test_results['eval_precision']:.4f}")
print(f"Recall: {test_results['eval_recall']:.4f}")
print(f"F1 Score: {test_results['eval_f1']:.4f}")

import numpy as np
from sklearn.metrics import classification_report
from seqeval.metrics import classification_report as seqeval_report
from seqeval.metrics import precision_score, recall_score, f1_score
import pandas as pd

# ---- Evaluate on test set ----
predictions, labels, _ = trainer.predict(test_dataset)

# Convert predictions to label IDs
preds = np.argmax(predictions, axis=2)

# Map predictions and labels to tag names (ignoring special tokens)
true_labels = [
    [id_to_label[label] for label in sent if label != -100]
    for sent in labels
]

true_preds = [
    [id_to_label[pred] for pred, label in zip(sent_pred, sent_label) if label != -100]
    for sent_pred, sent_label in zip(preds, labels)
]

# ---- Print Pretty Table ----
report_dict = seqeval_report(true_labels, true_preds, output_dict=True)
df = pd.DataFrame(report_dict).transpose()

# Keep only precision, recall, and f1-score columns
pretty_df = df[['precision', 'recall', 'f1-score']]
print(pretty_df)

"""# Load Model"""

import os
from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Define the path safely
model_path = os.path.join("/content", "drive", "MyDrive", "NLP", "my-ner-model")

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForTokenClassification.from_pretrained(model_path)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

print(next(model.parameters()).device)  # Should print: cuda:0
print(inputs['input_ids'].device)      # Should print: cuda:0

"""# Test on input"""

!pip install gradio
import torch
import spacy
from spacy import displacy
from spacy.tokens import Span
import re

def clean_text(text):
    # Remove mentions and URLs
    text = re.sub(r'@\w+|http\S+', '', text)

    # Remove hashtags and the hashtag words (e.g., "#vacation")
    text = re.sub(r'#\w+', '', text)

    # Normalize repeated characters (e.g., "Sooo" -> "So")
    text = re.sub(r'(.)\1{2,}', r'\1', text)

    # Remove emojis and special punctuation (keep only words and spaces)
    text = re.sub(r'[^\w\s]', '', text, flags=re.UNICODE)

    # Normalize whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    return text

# Initialize empty spaCy model for visualization
nlp = spacy.blank("en")

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def predict_ner_as_tagged_text(text):
    cleaned = clean_text(text)
    words = cleaned.split()

    encoded = tokenizer(words, is_split_into_words=True, return_tensors="pt",
                        truncation=True, padding="max_length", max_length=128)

    inputs = {k: v.to(device) for k, v in encoded.items()}

    with torch.no_grad():
        outputs = model(**inputs)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=2)[0].cpu().tolist()

    predicted_labels = []
    word_ids_list = tokenizer(words, is_split_into_words=True, return_tensors="pt").word_ids()
    previous_word_idx = None

    for idx, word_idx in enumerate(word_ids_list):
        if word_idx is None:
            continue
        elif word_idx != previous_word_idx:
            label_id = predictions[idx]
            label = id_to_label[label_id]

            if label != "O":
                predicted_labels.append((words[word_idx], label))

            previous_word_idx = word_idx

    doc = nlp(cleaned)
    ents = []
    offset = 0

    for word, label in predicted_labels:
        start = cleaned.find(word, offset)
        end = start + len(word)
        offset = end

        span = doc.char_span(start, end)
        if span:
            span = Span(doc, span.start, span.end, label)
            ents.append(span)

    doc.ents = ents

    colors = {
        "B-PER": "#ff4d4d",
        "B-LOC": "#3399ff",
        "B-ORG": "#33cc33",
        "B-MISC": "#ff9933",
        "I-PER": "#ff4d4d",
        "I-LOC": "#3399ff",
        "I-ORG": "#33cc33",
        "I-MISC": "#ff9933"
    }

    options = {"colors": colors}
    return displacy.render(doc, style="ent", jupyter=False, options=options)

"""# Gradio Interface"""

import gradio as gr
# Create Gradio Interface
iface = gr.Interface(
    fn=predict_ner_as_tagged_text,
    inputs=gr.Textbox(lines=2, placeholder="Enter text here..."),
    outputs="html",
    title="Named Entity Recognition System",
    description="Enter a sentence to get word-level NER tags."
)

if __name__ == "__main__":
    iface.launch(share=True, debug=True)